
Effective generalization in robotic manipulation requires representations that capture invariant patterns of interaction across environments and tasks. However, current Vision-Language-Action (VLA) models rely mainly on 2D inputs, neglecting the rich object structural information and commonsense knowledge inherent in the 3D physical world. This deficiency restricts their spatial awareness and adaptability for complex, high-precision manipulation. To bridge this crucial gap, we construct a Concept Expert module for VLA to build Analytic Concepts that represent objects as explicit, programmatic blueprints. Our mechanism operates in two synergistic phases: First, prior to VLA inference, the Concept Expert leverages 3D information from Vision Foundation Models (VFMs) to estimate the initial kinematic and structural parameters. Second, throughout the manipulation process, the VLA model utilizes its inherent capability to dynamically track the dynamic concept parameters, continuously aligning them with observational changes to ensure persistent accuracy. Once established, the Analytic Concepts provide explicit, high-quality guidance for VLA fine-tuning through (1) dense, programmatic manipulation rewards and (2) precise spatial guidance. This enables the VLA model to internalize structural and physical knowledge while retaining end-to-end learning flexibility. Experimental results demonstrate the universal efficacy of our approach, significantly enhancing Supervised Fine-tuning and Reinforcement Learning (RL) post-training for VLA models. This method leads to superior spatial awareness, quantifiable improvements in generalization to novel tasks, and consistently higher success rates across complex manipulation scenarios.